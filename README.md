# Meta-Word-Embeddings-using-Autoencoders
* Implementation of Meta-Word-Embeddings, a combination of word2vec, GloVe, and fassttext word embeddings using various types of autoencoders.
* Implementation based on the paper **"Learning Word Meta-Embeddings by Autoencoding"**. [[paper link]](https://www.aclweb.org/anthology/C18-1140/)
* The meta-embeddings are generating using three different variants of autoencoders namely:
  * Decopuled Autoencodeder (DAE)
  * Concatenated Autoencoder (CAE)
  * Averaged Autoencoder (AAE)
* Each of them is explained below.
